CST 3170 MACHINE LEARNING COURSEWORK REPORT
Digit Recognition System

Author: Dumitru Nirca
Date: December 2025
Course: CST 3170 Machine Learning

================================================================
SECTION 1. EXECUTIVE SUMMARY
================================================================
This document describes a digit recognition system that classifies
handwritten digits (UCI optical digit dataset) using a family of
algorithms ranging from 1-nearest neighbor to a Pegasos-trained
linear SVM. All code is packaged inside a single Java file to
match the coursework rules.

Latest two-fold results (January 2026 run):
  • Nearest Neighbor (k = 1): 98.26 %
  • Best k-NN per fold:       98.27 %
  • Weighted k-NN:            98.24 %
  • Linear SVM (Pegasos):     97.30 %

Training time for complete evaluation: approximately 7 hours.

The k-NN family delivers the highest accuracy, while the linear SVM
fulfills the “advanced algorithm” requirement and demonstrates the
hyperparameter analysis.

================================================================
SECTION 2. INTRODUCTION
================================================================
Problem Statement
  Build a machine learning pipeline that recognizes digits (0‑9)
  from 8×8 pixel grids. The solution must achieve strong accuracy
  and compare multiple algorithms.

Dataset
  Source: UCI Machine Learning Repository
  Format: CSV files, 64 numeric features + 1 label
  Classes: Ten digits (0 through 9)
  Features: 8×8 grayscale pixel intensities

System Architecture
  Models    : data structures for samples, predictions and results
  Algorithms: k-NN variants and linear SVM implementation
  Utilities : distance computations, evaluation metrics, loaders

================================================================
SECTION 3. ALGORITHMS IMPLEMENTED
================================================================
Nearest Neighbor Baseline
  • Simple Euclidean lookup implemented via KNearestNeighbors
    with k = 1.

k-Nearest Neighbors
  • Majority vote across k in {1, 3, 5, 7, 9, 11}.
  • Each fold evaluates every k and reports the best performer.

Weighted k-Nearest Neighbors
  • Same neighbor pool as standard k-NN.
  • Votes weighted by 1 / (distance + ε) to dampen noisy points.

Linear Support Vector Machine
  • One-vs-one linear SVM (pairwise classification with 45 binary
    classifiers) trained with Pegasos-style SGD.
  • Automatic sweep over regularization (λ), epoch count, learning
    rate floor, and shuffle strategy.
  • Feature engineering: spatial augmentation (row/column averages),
    Random Fourier Features (512 features) for RBF kernel approximation,
    and polynomial features (degree 2) for polynomial kernel approximation.
  • Ensemble of 5 models with voting for final predictions.
  • Demonstrates feature scaling, regularization and convergence
    monitoring.

================================================================
SECTION 4. DISTANCE METRICS
================================================================
  • Euclidean distance is the required baseline and the metric used
    for submitted scores.
  • Manhattan and Minkowski distances (order p) are available inside
    DistanceCalculator to support additional experiments without
    refactoring the pipeline.

================================================================
SECTION 5. EVALUATION METHODOLOGY
================================================================
  1. Two-fold cross-validation: Fold 1 trains on dataSet1.csv and
     tests on dataSet2.csv; Fold 2 swaps them.
  2. Metrics: overall accuracy, 10×10 confusion matrices, per-class
     precision/recall/F1, macro averages.
  3. Runtime hyperparameter selection:
       - k-NN evaluates every candidate value each fold.
       - Linear SVM shuffles, reserves an 85 % / 15 % validation
         split, and averages six repeats per hyperparameter set.

================================================================
SECTION 6. RESULTS AND ANALYSIS
================================================================
Two-Fold Summary (Latest Run)

Algorithm                     Fold 1 Accuracy   Fold 2 Accuracy   Average
------------------------------------------------------------------------
Nearest Neighbor (k = 1)      98.04 %           98.47 %           98.26 %
Best k-NN per fold            98.08 % (k = 3)   98.47 % (k = 1)   98.27 %
Weighted k-NN                 98.01 %           98.47 %           98.24 %
Linear SVM (Pegasos)          97.30 %           97.30 %           97.30 %

Note: Complete two-fold evaluation including SVM hyperparameter search and ensemble training required approximately 7 hours of computation time.

Hyperparameter Takeaways
  • The best k changes per fold; retuning avoids overfitting.
  • SVM uses comprehensive feature engineering: spatial augmentation
    adds row/column averages; Random Fourier Features (512 features)
    approximate RBF kernel; polynomial features (degree 2) capture
    feature interactions.
  • SVM accuracy benefits from λ values ranging from 0.0003 to 0.0012,
    combined with 100–240 epochs and minimum learning rates from
    5×10⁻⁸ to 1.5×10⁻⁷.
  • Ensemble of 5 models improves robustness by voting across multiple
    hyperparameter configurations.
  • Averaging six validation repeats per hyperparameter combination
    stabilizes SVM selection and prevents outlier splits from skewing
    the decision.
  • The comprehensive hyperparameter search with ensemble training
    achieves 97.30% accuracy but requires approximately 7 hours of
    computation time for complete two-fold evaluation.

Error Analysis
  • Most confusions occur between visually similar digits (1 vs 8,
    3 vs 5, 4 vs 9).
  • Linear SVM improves minority-class precision (≥ 0.89) but lags
    k-NN on curved digits.

================================================================
SECTION 7. CODE QUALITY AND IMPLEMENTATION
================================================================
  • All logic is contained in DigitRecognitionApp.java, partitioned
    into models, algorithms and utilities with clear comments.
  • The Classifier interface enforces consistent APIs; evaluation
    helpers centralize reporting.
  • Constants hold dataset paths, k grids and hyperparameter ranges,
    eliminating “magic numbers”.
  • Defensive programming includes null checks, normalization guards
    and informative logging.
  • Style follows JavaDoc conventions and remains readable despite
    the single-file constraint.

================================================================
SECTION 8. CONCLUSIONS
================================================================
Key Achievements
  1. Baseline requirement exceeded with 98.27 % average accuracy.
  2. Advanced algorithm implemented: Pegasos linear SVM achieving 97.30%
     accuracy through comprehensive hyperparameter search and ensemble
     training (7 hour computation time).
  3. Automated two-fold evaluation with confusion matrices and
     per-class summaries.
  4. High code quality maintained under single-file limitation.
  5. Documentation (report + README) captures methodology and future
     work.

Future Enhancements
  • Explore feature engineering (PCA, intensity statistics) before
    training the SVM.
  • Experiment with kernelized SVMs or shallow neural networks for
    higher “quality of algorithm” credit.
  • Persist evaluation summaries to CSV/text automatically to make
    reporting repeatable.

================================================================
SECTION 9. REFERENCES
================================================================
  1. UCI Machine Learning Repository – Optical Recognition of
     Handwritten Digits Dataset.
  2. Duda, R.O., Hart, P.E., and Stork, D.G. (2001). Pattern
     Classification. Wiley-Interscience.
  3. Hastie, T., Tibshirani, R., and Friedman, J. (2009). The Elements
     of Statistical Learning. Springer.

================================================================
SECTION 10. SELF-MARKING SHEET
================================================================
Component                 Max   Self-Mark   Rationale
--------------------------------------------------------------
Self-Marking Sheet        5     5           Included with full
                                              justification.
Running Code              10    10          Single Java file builds
                                              cleanly in Eclipse/CLI.
Two-Fold Test             5     5           Automated folds with
                                              complete reporting.
Quality of Code           20    18          Strong structure, naming
                                              and documentation.
Report                    20    10          Covers algorithms,
                                              hyperparameters,
                                              references.
Quality of Results        20    5          k-NN hits 98.27 %; SVM
                                              achieves 97.30 % with
                                              comprehensive tuning
                                              (7 hour runtime).
Quality of Algorithm      20    12         Multiple algorithms plus
                                              Pegasos SVM search.

TOTAL ESTIMATED SCORE: 92 / 100

================================================================
FINAL NOTE
================================================================
This report reflects the January 2026 evaluation and documents the
baseline k-NN variants alongside the advanced Pegasos linear SVM
implementation required for the CST 3170 coursework submission.
